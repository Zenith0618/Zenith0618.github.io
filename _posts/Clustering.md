---
layout:     post
title:      Clustering
subtitle:   Clustering
date:       2019-10-14
author:     ZXZ
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - learning
    - ML
---
## 聚类（Clustering）

### 一、性能度量

好的聚类结果应当是簇内相似度高，簇间相似度低。

#### 1. 外部指标 

将聚类结果与某个参考模型进行比较

- Jaccard系数
- FM指数
- Rand指数

#### 2. 内部指标

考虑聚类结果的簇划分$\{C_1,C_2,...,C_k\}$，$\mu_i$表示簇的样本中心点。定义
$$
avg(C) = {{2}\over{|C|(|C|-1)}} \sum_{1\le i \lt j \le |C|}dist(x_i,x_j) 
$$

$$
diam(C) = max_{1\le i \lt j \le |C|}dist(x_i,x_j)
$$

avg(C)表示簇C内所有样本间距离的平均值，它的值越小，说明簇内相似度越高。

diam(C)表示簇C内样本间距离的最大值，它的值越小，说明簇内相似度越高。
$$
d_{min}(C_i,C_j) = min_{x_i \in C_i,x_j \in c_j }dist(x_i,x_j)
$$

$$
d_{cen}(C_i,C_j) = dist(\mu_i,\mu_j)
$$

$d_{min}(C_i,C_j)$表示两个簇最近样本间的距离，它的值越大，说明簇间相似度越低。

$d_{cen}(C_i,C_j)$表示两个簇的中心点间的距离，它的值越大，说明簇间相似度越低。

- DB指数：
  $$
  DBI  = {1\over k}\sum_{i=1}^k \max_{j\neq i}({{avg(C_i)+avg(C_j)}\over {d_{cen}(C_i,C_j)}})
  $$
  DBI的值越小，聚类性能越好

- Dunn指数
  $$
  DI = \min_{1\le i \le k}\{\min_{j\ne i}({{d_{min}(C_i,C_j)}\over{max_{1\le l \le k}diam(C_l)}})\}
  $$
  DI的值越大，聚类性能越好

- 轮廓系数（Silhouette Coefficient)：

  对于每一个样本i，其轮廓系数定义为
  $$
  s(i) = {{b(i)-a(i)}\over {max\{b(i),a(i)\}}}
  $$
  其中，a表示当前样本到同簇内其他样本距离的平均值，b表示当前样本到最近邻簇（能使b的值最小的簇）的样本的距离的平均值。

  根据定义，
  $$
  s(i)\in [-1,1]
  $$
  s(i)接近1，说明样本聚类合理；
  
  s(i)接近-1，说明样本聚类可能出现错误；
  
  s(i)近似为0时，说明样本在两个簇的边界上。
  
  > https://blog.csdn.net/wangxiaopeng0329/article/details/53542606

### 二、距离计算

- **距离度量**$dist(.,.)$满足基本性质:

  - 非负性：$dist(.,.)\ge 0$
  - 同一性：$dist(x_i,x_j)=0$当且仅当$x_i=x_j$
  - 对称性：$dist(x_i,x_j)=dist(x_j,x_i)$
  - 直递性：$dist(x_i,x_j)\le dist(x_i,x_k) + dist(x_k,x_j)$

- **闵可夫斯基距离**：
  $$
  dist_{mk}(\mathbf{x_i,x_j}) = (\sum_{u=1}^n |x_{iu}-x_{ju}|^p)^{1\over p}
  $$

  - p=1,**曼哈顿距离**
  - p=2,**欧氏距离**
  - $p\to \infty$,**切比雪夫距离**

- 对无序属性可采用VDM（Value Difference Metric）进行度量。属性u上两个离散值a与b间的VDM距离定义为：
  $$
  VDM_p(a,b) = \sum_{i=1}^k|{{m_{u,a,i}}\over {m_{u,a}}}-{{m_{u,b,i}}\over{m_{u,b}}}|^p
  $$
  $m_{u,a}$表示在属性u上取值为a 的样本数，$m_{u,a,i}$表示在第i簇中属性u取值为a的样本数。

- 可采用**“非度量距离”**用于相似度的度量

### 四、原型聚类

#### 1. k均值算法（典型的EM算法）

- 输入：样本集$D=\{x_1,x_2,...,x_n\}$，聚类簇数$k$。

- 过程：

  1. 从D中随机选择k个样本作为初始均值向量（簇中心）

  2. 更新均值向量

     1）计算比较各个样本与各均值向量的距离，根据距离最近的原则将样本划入相应的簇中

     2） 根据各个簇中的样本计算均值向量，若发生改变，则更新均值向量

  3. 若各个均值向量均未更新或达到设定的最大轮数，则停止算法，将当前划分的簇作为输出结果

- 输出：簇$\{C_1,C_2,...,C_k\}$

#### 2. 学习向量量化（Learning Vector Quantization）

- 特点：LVQ假设数据样本带有类别标记，学习过程中利用类别标记来辅助聚类。

- 输入：样本集$D=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$，原型向量个数q，各个原型向量预设的类别标记$\{t_1,t_2,...,t_q\}$，学习率$\eta \in (0,1)$；

- 过程：

  1. 初始化一组原型向量$\{p_1,p_2,...,p_q\}$

  2. 迭代优化原型向量

     1）从样本集中随机选取一个样本$(x_i,y_i)$

     2）计算样本与各个原型向量间的距离，找出与样本距离最近的原型向量$p_{i^*}$；

     3） 判断$t_j$与$y_i$是否相同，若相同则按照式（1）更新该原型向量，否则按式（2）更新$p_{i^*}$。
     $$
     p_{i^*} = p_{i^*} + \eta.(x_i-p_{i^*}) \tag{1}
     $$

     $$
     p_{i^*} = p_{i^*} - \eta.(x_i-p_{i^*}) \tag{2}
     $$

  3. 达到停止条件前，一直进行步骤2.

- 输出：原型向量$\{p_1,p_2,...,p_q\}$

- 说明：每个原型向量定义了一个与之相关的区域$R_i$，由此形成对样本空间的簇划分。
  $$
  R_i = \{x \in \chi|\ ||x-p_i||_2 \le||x-p_j||_2,i \ne j \}
  $$
  

#### 3. 高斯混合聚类

- 思路：采用概率模型表达聚类原型

- **多元高斯分布：**对于n维样本空间中的随机向量$x$，若其服从高斯分布，其概率密度函数为：
  $$
  p(x|\mu,\Sigma)={1 \over {(2\pi)^{ n \over 2}|\Sigma|^{1\over 2}}}e^{-{1 \over 2}(x-\mu)^T\Sigma^{-1}(x-\mu)}
  $$
  $\mu$为$n$维均值向量，$\Sigma$为$n\times n$协方差矩阵。

- **高斯混合分布**：
  $$
  p_m(x) = \sum_{i=1}^k\alpha_ip(x|\mu_i,\Sigma_i)
  $$
  $\alpha_i\gt 0$为各个混合成分对应的混合系数，$\sum_{i=1}^k\alpha_i=1$。

- 分析：

  训练集$D=\{x_1,x_2,...,x_m\}$,令随机变量$z_j\in \{1,2,...,k\}$表示生成样本$x_j$的混合成分。根据贝叶斯定理，$z_j$的后验分布对应于：
  $$
  \gamma_{ji}=p_m(z_j=i|x_j)={{P(z_j=i)p_m(x_j|z_j=i)}\over{p_m(x_j)}}\\
  ={{P(z_j=i)p_m(x_j|z_j=i)}\over{\sum_{l=1}^k\alpha_l p(x_j|\mu_l,\Sigma_l)}}
  $$
    $\gamma_{ji}$给出了样本$x_j$由第$i$个混合成分生成的后验概率。		

    当高斯混合分布已知时，高斯混合聚类将样本集D划分为k个簇，每个样本的簇标记$\lambda_j$为：
  $$
  \lambda_j = arg\,\max_{i\in\{1,2,..,k\}}\gamma_{ji}
  $$
  **高斯混合聚类是采用概率模型对原型进行刻画，簇划分则由原型对应的后验概率确定。**

- 模型参数的求解：极大似然估计
  $$
  LL(D)=ln(\prod_{j=1}^mp_m(x_j))=\sum_{j=1}^mln(\sum_{l=1}^k\alpha_lp(x_j|\mu_l,\Sigma_l))
  $$

  $$
  \mu_i={{\sum_{j=1}^m\gamma_{ji}x_j} \over{\sum_{j=1}^m\gamma_{ji}}} \tag{1}
  $$

  各混合成分的均值可以通过样本加权平均来估计，权重为每个样本属于该混合成分的后验概率
  $$
  \Sigma_i={{\sum_{j=1}^m}\gamma_{ji}(x_j-\mu_i)(x_j-\mu_i)^T\over{\sum_{j=1}^m\gamma_{ji}}} \tag{2}
  $$

  $$
  \alpha_i={1\over m}\sum_{j=1}^m\gamma_{ji} \tag{3}
  $$

  各个混合系数由样本属于该成分的平均后验概率确定。

- 算法输入：样本集$D=\{x_1,x_2,...,x_m\}$；高斯混合成分个数$k$

- 过程：

  - 初始化高斯混合分布的模型参数$\{(\alpha_i,\mu_i,\Sigma_i)|1\le i\le k\}$

  - 满足停止条件前，使用EM算法迭代求解模型参数：

    1）**E**：计算所有样本由各混合成分生成的后验概率$\gamma_{ji},j=1,2,...,m.i=1,2,...,k$

    2）**M**：按照式（1）（2）（3）计算新的模型参数，并更新

  - 确定各个样本的簇标记，并划入相应的簇

- 算法输出：样本集的簇划分

### 五、密度聚类

#### 1.DBSCAN

- 概念：

  - $\epsilon-$邻域：对$x_j \in D$，其$\epsilon-$邻域包含样本集中与$x_j$的距离不大于$\epsilon$的样本，即$N_{\epsilon}(x_j) =\{x_i \in D|dist(x_j,x_i) \le \epsilon\}$
  - 核心对象：若$x_i$的$\epsilon-$邻域至少包含$MinPts$个样本，则$x_i$是一个核心对象
  - 密度直达：若$x_j \in N_{\epsilon}(x_i)$，且$x_i$是核心对象，那么称$x_j$由$x_i$密度直达
  - 密度可达：对于样本$x_i$和$x_j$，若存在样本序列$p_1,p_2,...,p_n$，且$p_1=x_i,p_n=x_j$，$p_{i+1}$能由$p_i$密度直达，那么称$x_j$由$x_i$密度可达
  - 密度相连：对于样本$x_i$和$x_j$，若存在$x_k$使得$x_i$和$x_j$均由$x_k$密度可达，则称$x_i$与$x_j$密度相连

- 簇：由密度可达关系导出的最大的密度相连样本集合，满足连接性（同一簇中任意两个样本密度相连）和最大性（与簇中某一样本密度相连的样本一定属于该簇）。

  由一个核心对象$x$密度可达的所有样本组成的集合$X=\{x^{'} \in D|x^{'}由x密度可达 \}$即为满足上述定义的簇。

- 算法输入：样本集$D=\{x_1,x_2,...,x_n\}$，邻域参数$(\epsilon,MinPts)$

- 过程：

  - 确定每个样本的$\epsilon-$邻域，进而找出样本集中的核心对象
  - 以任一核心对象为出发点，找出由其密度可达的所有样本生成聚类簇，直至访问完所有的核心对象为止

- 算法输出：簇划分$\{C_1,C_2,...,C_k\}$，注意簇的个数是在算法执行过程中确定的

  https://www.cnblogs.com/pinard/p/6217852.html

### 六、层次聚类

- 思路：在不同层次对数据集进行划分，从而形成树形的聚类结构。数据集的划分可采用“自底向上”的聚合策略，也可以采用“自顶向下”的分拆策略。

#### AGNES(Agglomerative Nesting)

- 概述：是一种采用“自底向上”聚合策略的层次聚类方法，先将数据集中的每个样本看作一个聚类簇，然后在算法运行中找出距离最近的两个聚类簇进行合并，该过程不断重复，直至达到预设的聚类簇个数。

- 算法输入：样本集D，聚类簇距离度量函数d，聚类簇个数k；

- 过程：

  - 初始化单样本聚类簇，聚类簇初始个数等于样本数

  - 初始化单样本聚类簇距离矩阵

  - 聚类簇个数大于k时：

    1）找出距离最近的两个聚类簇，并合并为一个聚类簇

    2）更新聚类簇距离矩阵

    3）更新聚类簇个数

  - 输出结果

- 算法输出：簇划分

- 聚类簇间的距离：
  - 最小距离：最近样本决定
  - 最大距离：最远样本决定
  - 平均距离：所有样本决定